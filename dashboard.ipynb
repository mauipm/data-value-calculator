{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf6d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import numpy as np\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "from ast import literal_eval\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "from sklearn.preprocessing import normalize # normalisation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from pyaltmetric import Altmetric # altmetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d46232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "ss_df_reduced = pd.read_csv(\"./exports/ss_df_reduced.csv\").drop(['influentialCitationCount'], axis=1)\n",
    "weights = pd.read_csv(\"./exports/weights.csv\")\n",
    "\n",
    "min_idx=0\n",
    "max_idx=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b9afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_df = pd.DataFrame({\"min\":ss_df_reduced.min(),\"max\":ss_df_reduced.max()}).T\n",
    "minmax_df.drop(\"paperId\", axis=1, inplace=True)\n",
    "#minmax_df[\"referenceCount\"][min_idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6a5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data nutzlos\n",
    "#dframe = pd.read_csv(\"citation_df.csv\")\n",
    "#df = dframe[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f18ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "colors = {\n",
    "    'background': '#041014',\n",
    "    'text': '#6674e4'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0451bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citations\n",
    "def extractCategory(row):\n",
    "    cat_list = []\n",
    "    if row != None:\n",
    "        for value in row:\n",
    "            cat_list.append(value.get('category'))\n",
    "    \n",
    "    return cat_list\n",
    "\n",
    "def getCitationDataFrame(paperId):\n",
    "    offset = 0\n",
    "    df = pd.DataFrame()\n",
    "    start_ts = datetime.now().replace(microsecond=0)\n",
    "\n",
    "    while offset != None:\n",
    "        try:\n",
    "            # print('Offset: '+str(offset))\n",
    "            req = requests.get(\"https://api.semanticscholar.org/graph/v1/paper/\"+paperId+\"/citations?fields=title,referenceCount,citationCount,influentialCitationCount,isInfluential,fieldsOfStudy,s2FieldsOfStudy,year,publicationDate&limit=1000&offset=\"+str(offset)).json()\n",
    "        \n",
    "            # Wait 5 Minutes and retry if API-Server sends 'Too Many Requests'\n",
    "            while req.get('message') == 'Too Many Requests':\n",
    "                print('Waiting 5 minutes.')\n",
    "                time.sleep((60*5)+5) # Wait 5 Minutes + 5 Extra-seconds for tolerance\n",
    "                req = requests.get(\"https://api.semanticscholar.org/graph/v1/paper/\"+paperId+\"/citations?fields=title,referenceCount,citationCount,influentialCitationCount,isInfluential,fieldsOfStudy,s2FieldsOfStudy,year,publicationDate&limit=1000&offset=\"+str(offset)).json()\n",
    "            \n",
    "            df = pd.concat([df, pd.DataFrame(req.get('data'))])\n",
    "            offset = req.get('next')\n",
    "        except:\n",
    "            print('An error was returned from the API-Server while querying the paper with paperId: ' + str(paperId) + '.. Proceeding with next row.')\n",
    "        \n",
    "    df = df[['isInfluential']].reset_index(drop=True).join(pd.json_normalize(df['citingPaper']))\n",
    "    df['s2FieldsOfStudy'] = df['s2FieldsOfStudy'].apply(lambda x: extractCategory(x))\n",
    "\n",
    "    # df['fieldsOfStudy'] = [ [] if x is None else x for x in df['fieldsOfStudy'] ] + df['s2FieldsOfStudy']\n",
    "    df['fieldsOfStudy'] = [ [] if x is None or x is np.NaN else x for x in df['fieldsOfStudy'] ]\n",
    "\n",
    "    # Combine foS & s2FoS\n",
    "    df['fieldsOfStudy'] = df['fieldsOfStudy']+df['s2FieldsOfStudy']\n",
    "\n",
    "    # Remove duplicates from the created list\n",
    "    df['fieldsOfStudy'] = df['fieldsOfStudy'].apply(lambda row: sorted(list(dict.fromkeys(row))))\n",
    "\n",
    "    # Remove rows with missing Ids\n",
    "    if df[df['citationCount'].isna()].shape[0]>0:\n",
    "        print('Removed ' + str(df[df['citationCount'].isna()].shape[0]) + ' rows which had missing paperId and external Ids.')\n",
    "        df = df[~df['citationCount'].isna()]\n",
    "\n",
    "    # Dtype conversion\n",
    "    # df['year'] = df['year'].astype(int)\n",
    "    df['referenceCount'] = df['referenceCount'].astype(int)\n",
    "    df['citationCount'] = df['citationCount'].astype(int)\n",
    "    df['influentialCitationCount'] = df['influentialCitationCount'].astype(int)\n",
    "\n",
    "    end_ts = datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    print('>>> Received ' + str(df.shape[0]) + ' rows [Total time elapsed: ' + str((end_ts-start_ts)) + ']')\n",
    "    df.insert(0, 'Ref_paperId', paperId)\n",
    "    \n",
    "    return df.drop(['s2FieldsOfStudy'], axis=1)\n",
    "    # return df\n",
    "    # .insert(loc=0, column='Ref_paperId', value=paperId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2f554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAMdata(paper_doi):\n",
    "\n",
    "    try:\n",
    "        # Altmetric-Objekt erstellen\n",
    "        a = Altmetric()\n",
    "\n",
    "        # In Altmetric-DB nach Paper mit entsprechender DOI suchen (API antwortet mit JSON-Array)\n",
    "        altmetric_json_data = a.doi(paper_doi)\n",
    "\n",
    "        # DataFrame erstellen und in geeignetes Format transformieren\n",
    "        df = pd.DataFrame.from_dict(altmetric_json_data.items()).transpose()\n",
    "\n",
    "        # DataFrame filtern\n",
    "        df.columns = df.iloc[0]\n",
    "\n",
    "        # Altmetric Score aufrunden\n",
    "        df = df.iloc[1]\n",
    "        df['score'] = np.ceil(df['score']).astype(int)\n",
    "\n",
    "        # Alle Spalten extrahieren, die 'count' oder/und 'score' enthalten (z.B. gibt es nicht bei jedem Paper die Information wie viele Reddit, Wikipedia etc. Einträge vorhanden sind)\n",
    "        col_list = []\n",
    "        for col in df.keys():\n",
    "            if any(x in col for x in ['count', 'score', 'doi']):\n",
    "                col_list.append(col)\n",
    "\n",
    "        # DataFrame nach den extrahierten Spalten filtern\n",
    "        df = df[col_list]\n",
    "\n",
    "        # Spalten umbennenen, damit zum Schluß nach dem Konkatenieren leichter nachvollzogen werden kann, aus welcher Quelle die Daten stammen.\n",
    "        for col in df.keys():\n",
    "            if col in col_list:\n",
    "                df.rename({col:'altmetric_'+col}, inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # return pd.DataFrame()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5566ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere Journalseiten, welche durch das Paper eingenommen werden\n",
    "def extractOccupiedJournalPagesCount(journalPages):\n",
    "    # Belegte Seiten des Journals in dem das Paper veröffentlicht wurde berechnen\n",
    "    # Manche Angaben werden in ihrer Schreibweise verkürzt... z.B.: 153-57, dies soll wie 153-157 behandelt werden, da sonst negative Zahlen bei der folgenden Berechnung entstehen.\n",
    "\n",
    "    try:\n",
    "        split_pages = journalPages.strip().replace('–', '-').split('-')\n",
    "\n",
    "        try:\n",
    "            page_from = split_pages[0]\n",
    "            page_to = split_pages[1]\n",
    "            pagesOccupiedCount = int(page_to)-int(page_from)\n",
    "\n",
    "            if pagesOccupiedCount<0:\n",
    "                # Seitenzahl ist negativ (Seitenzahl von page_from war größer als Seitenzahl von page_to!)\n",
    "                digit_delta = len(page_from)-len(page_to)\n",
    "                # Z.B.: 133-38 wird geschaut aus wie vielen Zahlen beide aufgespaltenen Strings bestehen. Es wird die Länge verglichen, in dem Beispiel hat Zahl an [Index 0] 1 Stelle mehr als die Zahl an [Index 1].\n",
    "                # Es wird nun von der Zahl an [Index 0] der Differenz entsprechend viele Zahlen von Links nach Rechts entnommen und an die Zahl an [Index 1] vorne angefügt.\n",
    "                # So wird aus der Seitenzahl 38 nach diesem Muster [1]33 - [1]38 (die 1 der ersten Zahl wird vorne an die zweite Zahl angefügt).\n",
    "                page_fixed = page_from[0:digit_delta]+page_to\n",
    "                pagesOccupiedCount = int(page_fixed)-int(page_from)\n",
    "                return pagesOccupiedCount\n",
    "            else:\n",
    "                return pagesOccupiedCount\n",
    "                \n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ce2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSdata(paperId):\n",
    "  \n",
    "  #######################\n",
    "  #     SEND REQUEST    #\n",
    "  #######################\n",
    "  try:\n",
    "    api_key = 'rGUKYOEpCP2FKQK88CLuB1izZBvDiQwA5SsSZ5vo'\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/\"+paperId.replace(' ', '').strip()+\"?fields=title,referenceCount,citationCount,influentialCitationCount,externalIds,authors.hIndex,authors.paperCount,authors.citationCount,authors.affiliations,authors.paperCount,authors.name,fieldsOfStudy,publicationTypes,publicationDate,year,journal,isOpenAccess\"\n",
    "    req = requests.get(url=url, params={'x-api-key': api_key}).json()\n",
    "\n",
    "    # Wait 5 Minutes and retry if API-Server sends 'Too Many Requests'\n",
    "    while req.get('message') == 'Too Many Requests':\n",
    "      print('[Too Many Requests]: Waiting 5 minutes.')\n",
    "      time.sleep((60*5)+5) # Wait 5 Minutes + 5 Extra-seconds for tolerance\n",
    "      req = requests.get(url=url, params={'x-api-key': api_key}).json()\n",
    "\n",
    "    ############################\n",
    "    #     PROCESS RESPONSE     #\n",
    "    ############################\n",
    "    df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in req.items() ]));\n",
    "\n",
    "    # Features aus verschachteltem JSON-Array extrahieren und in separate Spalten teilen\n",
    "    for col in ['DOI', 'CorpusId']:\n",
    "      try:\n",
    "        df[col] = df.loc[col].externalIds\n",
    "      except:\n",
    "        df[col] = np.nan\n",
    "    \n",
    "    df['paperId'] = df['paperId'].iloc[0]\n",
    "    df['title'] = df['title'].iloc[0]\n",
    "    df = df[df['externalIds'].isna()]\n",
    "    df['journal_name'] = df.loc['name'].journal if df.loc['name'].journal else np.nan\n",
    "\n",
    "    # Belegte Seiten des Journals in dem das Paper veröffentlicht wurde berechnen\n",
    "    try:\n",
    "        # Manche Angaben werden in ihrer Schreibweise verkürzt... z.B.: 153-57, dies soll wie 153-157 behandelt werden, da sonst negative Zahlen bei der folgenden Berechnung entstehen.\n",
    "        df['occupiedJournalPages'] = extractOccupiedJournalPagesCount(df.loc['pages'].journal)\n",
    "    except:\n",
    "        df['occupiedJournalPages'] = 0\n",
    "\n",
    "    # MV filtern\n",
    "    df = df[~df['authors'].isna()][['DOI', 'CorpusId', 'paperId', 'title', 'referenceCount', 'citationCount', 'influentialCitationCount', 'fieldsOfStudy', 'publicationTypes', 'journal_name', 'occupiedJournalPages', 'isOpenAccess', 'publicationDate', 'year', 'authors']]\n",
    "\n",
    "    # Author Name extrahieren\n",
    "    df['author_names'] = df['authors'].apply(lambda x: x['name'])\n",
    "    df['author_names'] = str(df['author_names'].to_list())\n",
    "    df['author_names'] = df['author_names'].apply(literal_eval)\n",
    "\n",
    "    # # Author Affiliation extrahieren\n",
    "    # df['authors_affiliations'] = df['authors'].apply(lambda x: x['affiliations'])\n",
    "    # df['authors_affiliations'] = str(df[~df['authors_affiliations'].isna()]['authors_affiliations'].to_list())\n",
    "    # df['authors_affiliations'] = df['author_affiliations'].apply(literal_eval)\n",
    "\n",
    "    # h-Index aller Autoren extrahieren\n",
    "    df['hIndex'] = df['authors'].apply(lambda x: x['hIndex'])\n",
    "\n",
    "    # Avg, min, max h-Index über alle Autoren berechnen, runden und in Ganzzahl umwandeln\n",
    "    df['authors_sum_hIndex'] = df['hIndex'].sum().round(0).astype(int)\n",
    "    # df['authors_mean_hIndex'] = df['hIndex'].mean().round(0).astype(int)\n",
    "    # df['authors_max_hIndex'] = df['hIndex'].max().round(0).astype(int)\n",
    "    # df['authors_min_hIndex'] = df['hIndex'].min().round(0).astype(int)\n",
    "\n",
    "    # paperCount aller Autoren extrahieren\n",
    "    df['author_paperCount'] = df['authors'].apply(lambda x: x['paperCount'])\n",
    "\n",
    "    # Avg, min, max paperCount über alle Autoren berechnen, runden und in Ganzzahl umwandeln\n",
    "    df['authors_sum_paperCount'] = df['author_paperCount'].sum().round(0).astype(int)\n",
    "    # df['authors_mean_paperCount'] = df['author_paperCount'].mean().round(0).astype(int)\n",
    "    # df['authors_max_paperCount'] = df['author_paperCount'].max().round(0).astype(int)\n",
    "    # df['authors_min_paperCount'] = df['author_paperCount'].min().round(0).astype(int)\n",
    "\n",
    "    # citationCount aller Autoren extrahieren\n",
    "    df['author_citationCount'] = df['authors'].apply(lambda x: x['citationCount'])\n",
    "\n",
    "    # Avg, min, max citationCount über alle Autoren berechnen, runden und in Ganzzahl umwandeln\n",
    "    df['authors_sum_citationCount'] = df['author_citationCount'].sum().round(0).astype(int)\n",
    "    # df['authors_mean_citationCount'] = df['author_citationCount'].mean().round(0).astype(int)\n",
    "    # df['authors_max_citationCount'] = df['author_citationCount'].max().round(0).astype(int)\n",
    "    # df['authors_min_citationCount'] = df['author_citationCount'].min().round(0).astype(int)\n",
    "\n",
    "    df['authorsCount'] = len(df['author_names'])\n",
    "\n",
    "    # Redundante bzw. irrelevante Features entfernen\n",
    "    df = df.iloc[0][['DOI', 'CorpusId', 'paperId', 'title', 'author_names', 'isOpenAccess', 'publicationTypes', 'journal_name', 'occupiedJournalPages', 'publicationDate', 'year', 'fieldsOfStudy', 'referenceCount', 'citationCount', 'authorsCount', 'authors_sum_hIndex', 'authors_sum_paperCount', 'authors_sum_citationCount']]\n",
    "\n",
    "    # Datentyp in Ganzzahl umwandeln\n",
    "    df['referenceCount'] = df['referenceCount'].astype(int)\n",
    "    df['citationCount'] = df['citationCount'].astype(int)\n",
    "    # df['influentialCitationCount'] = df['influentialCitationCount'].astype(int)\n",
    "    if df['year']:\n",
    "      df['year'] = df['year'].astype(int)\n",
    "\n",
    "    # print('- Collected Semantic Scholar metadata.')\n",
    "    return df\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d516765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get paper information(Abfrage an Sementic, Altmetric, IEEE)\n",
    "def getNewPaperWithDOI(input_string_doi):\n",
    "    ss_df = pd.DataFrame(getSSdata(input_string_doi)).T\n",
    "    try:\n",
    "        al_df = pd.DataFrame(getAMdata(input_string_doi)).T\n",
    "        ss_df = ss_df.merge(al_df, how='left', left_on='DOI', right_on='altmetric_doi').drop(['altmetric_doi'], axis=1)\n",
    "    except:\n",
    "        ss_df['altmetric_score'] = 0\n",
    "    \n",
    "    return ss_df\n",
    "\n",
    "\n",
    "#normalization veraltet\n",
    "def normaliseDataFrame(ss_df_reduced):\n",
    "    df_normalized = normalize(ss_df_reduced[['referenceCount', 'citationCount', 'authorsCount','occupiedJournalPages', 'authors_sum_paperCount',\n",
    "       'authors_sum_citationCount', 'authors_sum_hIndex', 'altmetric_score']])\n",
    "\n",
    "    return ss_df_reduced[['paperId']].join(df_normalized)\n",
    "def normalize(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "    return pd.DataFrame(scaler.fit_transform(data))\n",
    "\n",
    "\n",
    "def calScore(npa):\n",
    "    score = float(0.0)\n",
    "    try:\n",
    "        score += weights.referenceCount * ((npa[\"referenceCount\"][0] - minmax_df[\"referenceCount\"][min_idx])\n",
    "                                           /(minmax_df[\"referenceCount\"][max_idx]-minmax_df[\"referenceCount\"][min_idx]))\n",
    "        # print(score)\n",
    "        score += weights.citationCount * ((npa[\"citationCount\"][0] - minmax_df[\"citationCount\"][min_idx])\n",
    "                                           /(minmax_df[\"citationCount\"][max_idx]-minmax_df[\"citationCount\"][min_idx]))\n",
    "        # print(score)\n",
    "        score += weights.authorsCount * ((npa[\"authorsCount\"][0] - minmax_df[\"authorsCount\"][min_idx])\n",
    "                                           /(minmax_df[\"authorsCount\"][max_idx]-minmax_df[\"authorsCount\"][min_idx]))\n",
    "        # print(score)\n",
    "        score += weights.occupiedJournalPages * ((npa[\"occupiedJournalPages\"][0] - minmax_df[\"occupiedJournalPages\"][min_idx])\n",
    "                                           /(minmax_df[\"occupiedJournalPages\"][max_idx]-minmax_df[\"occupiedJournalPages\"][min_idx]))\n",
    "        # print(score)\n",
    "        score += weights.authors_sum_paperCount * ((npa[\"authors_sum_paperCount\"][0] - minmax_df[\"authors_sum_paperCount\"][min_idx])\n",
    "                                           /(minmax_df[\"authors_sum_paperCount\"][max_idx]-minmax_df[\"authors_sum_paperCount\"][min_idx]))\n",
    "        # print(score)\n",
    "        score += weights.authors_sum_citationCount * ((npa[\"authors_sum_citationCount\"][0] - minmax_df[\"authors_sum_citationCount\"][min_idx])\n",
    "                                           /(minmax_df[\"authors_sum_citationCount\"][max_idx]-minmax_df[\"authors_sum_citationCount\"][min_idx]))\n",
    "        # print(score)\n",
    "        score += weights.authors_sum_hIndex * ((npa[\"authors_sum_hIndex\"][0] - minmax_df[\"authors_sum_hIndex\"][min_idx])\n",
    "                                           /(minmax_df[\"authors_sum_hIndex\"][max_idx]-minmax_df[\"authors_sum_hIndex\"][min_idx]))\n",
    "        # print(score)\n",
    "        try:\n",
    "            score += weights.altmetric_score * ((npa[\"altmetric_score\"][0] - minmax_df[\"altmetric_score\"][min_idx])\n",
    "                                            /(minmax_df[\"altmetric_score\"][max_idx]-minmax_df[\"altmetric_score\"][min_idx]))\n",
    "        # print(score)\n",
    "        except:\n",
    "            pass\n",
    "        # print(npa)\n",
    "    except Exception as e:\n",
    "        # print(\"################################\")\n",
    "        print(e)\n",
    "        #multiply time 1000 cause of the minmax scaling\n",
    "    return score*1000\n",
    "\n",
    "#generate table\n",
    "def generate_table(dataframe, max_rows=10):\n",
    "    return html.Table([\n",
    "        html.Thead(\n",
    "            html.Tr([html.Th(col) for col in dataframe.columns])\n",
    "        ),\n",
    "        html.Tbody([\n",
    "            html.Tr([\n",
    "                html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "            ]) for i in range(min(len(dataframe), max_rows))\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "def generateFigure(doi):\n",
    "    ss_df_nfig = pd.DataFrame(getSSdata(doi)).T\n",
    "    \n",
    "    paperCitations = getCitationDataFrame(doi)\n",
    "    paperCitationsExploded = paperCitations.explode('fieldsOfStudy')\n",
    "    \n",
    "    newPFOS = paperCitationsExploded[paperCitationsExploded.year >= ss_df_nfig.year[0]].groupby('fieldsOfStudy')['fieldsOfStudy'].count().reset_index(name = 'CitationCount')\n",
    "    newPYC = paperCitations[paperCitations.year >= ss_df_nfig.year[0]].groupby('year')['year'].count().reset_index(name = 'CitationCount')\n",
    "    # print(newPYC)\n",
    "    # print(\"new paper field of study\")\n",
    "    # print(newPFOS)\n",
    "    figcitationcount = px.bar(newPYC, x='year', y='CitationCount', height=400)\n",
    "    figfos = px.bar(newPFOS, x='fieldsOfStudy', y='CitationCount', height=400)\n",
    "    print(\"exit generate fig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42cb3625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14441\n",
       "Name: citationCount, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npfirst = getNewPaperWithDOI('DOI:10.1001/jama.2020.2648')\n",
    "npfirst[\"citationCount\"]\n",
    "#npfirst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a26402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_paper_=npfirst[minmax_df.columns]\n",
    "#new_paper_[\"citationCount\"][0]\n",
    "#nsa = calScore(new_paper_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79f839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ff2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c86c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 rows which had missing paperId and external Ids.\n",
      ">>> Received 838 rows [Total time elapsed: 0:00:02]\n"
     ]
    }
   ],
   "source": [
    "paperCitations = getCitationDataFrame('10.1002/wps.20238')\n",
    "paperCitationsExploded = paperCitations.explode('fieldsOfStudy')\n",
    "\n",
    "#paperCitationsExploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7457816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015\n",
       "Name: year, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_df_fig = pd.DataFrame(getSSdata('10.1002/wps.20238')).T\n",
    "ss_df_fig.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab470582",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfos = paperCitationsExploded[paperCitationsExploded.year >= ss_df_fig.year[0]].groupby('fieldsOfStudy')['fieldsOfStudy'].count().reset_index(name = 'CitationCount')\n",
    "# pcfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dddd8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcyear = paperCitations[paperCitations.year >= ss_df_fig.year[0]].groupby('year')['year'].count().reset_index(name = 'CitationCount')\n",
    "#pcyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67481a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(getSSdata('9d0c4389436d3ab8ed329dba8c58e8ac6737fd3b')).T\n",
    "#ss_df_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74254ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss_df = pd.DataFrame(getSSdata('10.1002/wps.20238')).T\n",
    "#al_df = pd.DataFrame(getAMdata('10.1002/wps.20238'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0410a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "npfirst = getNewPaperWithDOI('10.1002/wps.20238')\n",
    "#npfirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99423819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citationCount per year\n",
    "figcitationcount = px.bar(pcyear, x='year', y='CitationCount', height=400)\n",
    "figfos = px.bar(pcfos, x='fieldsOfStudy', y='CitationCount', height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4898cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# launch application\n",
    "\n",
    "app.layout = html.Div([\n",
    "    \n",
    "  #  html.Div(children=[ html.Img(src='assets/oarilogo.png', style={'height': '60px','width':'60px'}), \n",
    "  #              html.H2(children='Online Attention Research Impact (OARI)', style={'textAlign': 'center', 'color': colors['text']})\n",
    "   #                    \n",
    "  #   ], style={'display': 'flex', 'justifyContent': 'space-between', 'alignItems': 'center'}),\n",
    "    #html.Img(src='assets/oarilogo.png', style={'height': '65px','position':'absolute','top':'10px','right':'100px'}),\n",
    "    #html.Img(src='assets/frauaslogo.png', style={'height': '65px','position':'absolute','top':'10px','right':'300px'}),\n",
    " \n",
    "    html.Div([\n",
    "          html.Div([\n",
    "             html.Img(src='assets/oarilogo.png', style={'height': '65px','width':'65px'})\n",
    "          ],style={'alignItems':'center'}), \n",
    "         \n",
    "          html.Div([\n",
    "             html.H2(children='Online Attention Research Impact (OARI)', style={'textAlign': 'center', 'margin':'5px', 'color': colors['text']})\n",
    "          ],style={'alignItems':'center'}), \n",
    "        html.Div([\n",
    "             html.Img(src='assets/frauaslogo.png', style={'height': '65px'})\n",
    "          ],style={'alignItems':'center'}),\n",
    "                       \n",
    "     ], style={'display': 'flex', 'justifyContent': 'center', 'alignItems': 'center'}),\n",
    "    \n",
    "    html.Div([\n",
    "    html.Div([\n",
    "    \n",
    "        html.Div([\n",
    "            \"Input: \",\n",
    "            dcc.Input(id='input-string-doi', type='text', value='10.1002/wps.20238',debounce=True,style={'width':'250px','border': '3px solid #F9F7F9'}),\n",
    "            html.Br(),\n",
    "            \n",
    "            html.Div(id='output-string'),\n",
    "        ],style={'text-align':'center'}),\n",
    "        \n",
    "        html.Div([\n",
    "         dcc.Loading(\n",
    "            id=\"loading-1\",\n",
    "            type=\"default\",\n",
    "            children=[html.Div(id='output-1')]\n",
    "        )\n",
    "        ],style={'text-align':'center'}),\n",
    "\n",
    "        html.Div([\n",
    "         html.H2(\"Weight Information\", style={'text-align': 'center', 'color': 'blue'}),\n",
    "\n",
    "\n",
    "            html.P(\"Reference Count: {}\".format(weights.iloc[0]['referenceCount'])),\n",
    "            html.P(\"Citation Count: {}\".format(weights.iloc[0]['citationCount'])),\n",
    "            html.P(\"Authors Count: {}\".format(weights.iloc[0]['authorsCount'])),\n",
    "            html.P(\"Occupied Journal Pages: {}\".format(weights.iloc[0]['occupiedJournalPages'])),\n",
    "            html.P(\"Authors Sum of Paper Count: {}\".format(weights.iloc[0]['authors_sum_paperCount'])),\n",
    "            html.P(\"Authors Sum of Citation Count: {}\".format(weights.iloc[0]['authors_sum_citationCount'])),\n",
    "            html.P(\"Authors Sum of H-Index: {}\".format(weights.iloc[0]['authors_sum_hIndex'])),\n",
    "            html.P(\"Altmetric Score: {}\".format(weights.iloc[0]['altmetric_score'])),\n",
    "           ], style={'marginBottom': 50, 'marginTop': 25,'text-align': 'left',\n",
    "      'border': '3px solid #F9F7F9'}),\n",
    "        html.Br()\n",
    "    ]), \n",
    "    \n",
    "    html.Div([\n",
    "         \n",
    "        #html.H4(children='Figure 1', style={'textAlign': 'center'),\n",
    "        dcc.Graph(id='citation-count-per-year', figure=figcitationcount),\n",
    "        dcc.Graph(id='fieldofstudyperyear', figure=figfos)\n",
    "    \n",
    "    ]),\n",
    "   ], style={'display': 'flex', 'border':'3px solid #F9F7F9'}),\n",
    "\n",
    "    generate_table(ss_df_reduced)\n",
    "], style={'background':'white','font-family': 'Segoe UI'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e92e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(Output(component_id='output-string', component_property='children'),\n",
    "              [Input(component_id='input-string-doi', component_property='value')])\n",
    "def update_output_string(input_string_doi):\n",
    "    try:\n",
    "        new_paper = getNewPaperWithDOI(input_string_doi) #getPaper\n",
    "        #print(new_paper)\n",
    "        npa = new_paper[minmax_df.columns]\n",
    "        \n",
    "        npa = npa.fillna(0)\n",
    "       \n",
    "        score = calScore(npa)\n",
    "        \n",
    "        #generateFigure(input_string_doi)\n",
    "        \n",
    "        print('Score was Calculated: ' + str(score[0]))\n",
    "        return \"Output: OARI-Index is \"+str(math.ceil(score[0]))\n",
    "    except Exception as e:\n",
    "        print('Exception in \"update_out_string!\"')\n",
    "        print(e)\n",
    "        return 'The DOI could not be found, please try another one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3351948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(Output(component_id='citation-count-per-year', component_property='figure'),\n",
    "              Output(component_id='fieldofstudyperyear', component_property='figure'),\n",
    "              [Input(component_id='input-string-doi', component_property='value')])\n",
    "def update_figure(input_string_doi):\n",
    "   \n",
    "    ss_df_nfig = pd.DataFrame(getSSdata(input_string_doi)).T\n",
    "    \n",
    "    paperCitations = getCitationDataFrame(input_string_doi)\n",
    "    paperCitationsExploded = paperCitations.explode('fieldsOfStudy')\n",
    "    \n",
    "    newPFOS = paperCitationsExploded[paperCitationsExploded.year >= ss_df_nfig.year[0]].groupby('fieldsOfStudy')['fieldsOfStudy'].count().reset_index(name = 'CitationCount')\n",
    "    newPFOS.sort_values(\"CitationCount\", ascending = True, inplace=True)\n",
    "    \n",
    "    newPYC = paperCitations[paperCitations.year >= ss_df_nfig.year[0]].groupby('year')['year'].count().reset_index(name = 'CitationCount')\n",
    "   # print(newPYC)\n",
    "    figyearc = px.bar(newPYC, x='year', y='CitationCount', height=400, title=\"Citation count per Year\")\n",
    "    figfos = px.bar(newPFOS, x='fieldsOfStudy', y='CitationCount', color=\"CitationCount\", height=400, title=\"Fields of Studies where Paper was cited\")\n",
    "    \n",
    "    print(\"EXITING THE FIGURE UPDATE\")\n",
    "    return figyearc, figfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c058c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ff3edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.callback(Output(component_id='output-1', component_property='children'),\n",
    "              [Input(component_id='input-string-doi', component_property='value')])\n",
    "def update_output(input_string_doi):\n",
    "    # print(\"enter in info call1\")\n",
    "    ss_info_df = pd.DataFrame(getSSdata(input_string_doi)).T\n",
    "    # print(ss_info_df)\n",
    "    # print(\"enter in info call2\")\n",
    "    return html.Div([\n",
    "        html.H2(\"Paper Information\", style={'text-align': 'center', 'color': 'blue'}),\n",
    "        html.P('Paper Id: {}'.format(ss_info_df['paperId'][0])),\n",
    "        html.P('Title: {}'.format(ss_info_df['title'][0])),\n",
    "        html.P('Names of Authors: {}'.format(\", \".join(ss_info_df['author_names'][0]))),\n",
    "        html.P('Published in: {}'.format(ss_info_df['year'][0])),\n",
    "        html.P('Fields Of Study: {}'.format(ss_info_df['fieldsOfStudy'][0])),\n",
    "        html.P('Reference Count: {}'.format(ss_info_df['referenceCount'][0]))\n",
    "    ], style={'marginBottom': 50, 'marginTop': 25,'text-align': 'left',\n",
    "      'border': '3px solid #F9F7F9'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f972848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8050\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:13] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:14] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:14] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:14] \"GET /assets/oarilogo.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:14] \"GET /assets/frauaslogo.png HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:14] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:14] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:15] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:15] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was Calculated: 8.892454820688839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Feb/2023 17:31:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 rows which had missing paperId and external Ids.\n",
      ">>> Received 838 rows [Total time elapsed: 0:00:02]\n",
      "EXITING THE FIGURE UPDATE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Feb/2023 17:31:28] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Feb/2023 17:31:29] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was Calculated: 1342.4277377312058\n",
      "Removed 5 rows which had missing paperId and external Ids.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Feb/2023 17:32:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Received 8995 rows [Total time elapsed: 0:00:58]\n",
      "EXITING THE FIGURE UPDATE\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b117503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488ac61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8c798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40cbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966ec14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbd65fc690d3685c1b6e92b591b5d58183363517bb21956099026d3ab3ce8bce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
